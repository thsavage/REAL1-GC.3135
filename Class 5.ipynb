{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes 5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Multiple Features and the Valid Application of Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is trivial to extend the single-feature linear model to a linear model that simultaneously incorporates multiple features.  The interpretation of the results of a statistical model that uses multiple features is the same the interpretation of the partial derivative from the calculus of many variables: the effect of a small change in a particular feature on a label (or outcome).  \n",
    "\n",
    "A model with $K$ features, $x_{ik}$ and label $y_i$:\n",
    "\n",
    "$y_i=\\sum_{k=1}^Kx_{ik}\\cdot\\beta_k+\\epsilon_i = x_i^\\prime \\beta + \\epsilon_i$\n",
    "\n",
    "The $K$ features $x_{ik}$ influence the label $y_i$ through the $K$-vector, $\\beta$, which we estimate statistically.  A specific partial derivative interpretation.\n",
    "\n",
    "${\\displaystyle \\frac{\\partial E(y_i)}{\\partial x_{ik}}=\\beta_k}$\n",
    "\n",
    "For those interested in ancient history: Frisch–Waugh–Lovell theorem.  One can applied the basic principle from multivariate calculus: Holding everything else constant, what is the impact of a feature on an outcome of interest.\n",
    "\n",
    "**Bottom line is simple**: Fit the linear model with multiple features.  The basic approach to hypothesis testing remains unchanged.  The challenge is the interpretation of the results and the valid application of regression diagnostics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit: R$^2$\n",
    "\n",
    "Different statisical computing environments largely produce the same regression ouput, formatted differently.  This information is typically used for 'regression diagnosgics.'  We have begun to cover regression coefficients and their interpretation, as well as the use of standard errors, t values, and confidence intervals for hypothesis testing. \n",
    "\n",
    "The R$^2$ 'goodness of fit' metric is a frequently-cited regression diagnostic.  If a linear regression uses a constant (which should be included in practice), the R$^2$ is bounded between 0 and 1.  It measures the share of the variation in $y$ explained by the variation in the features used in a model.  Given this definition, 'bigger is better' is the first place that people go to evaluate the quality of the model, which is unwarranted.  \n",
    "\n",
    "> \"However, it can still be challenging to determine what is a good R$^2$ value, and in general, this will depend on the application.  For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error.  In this case, we would expect to see an R$^2$ value that is extremely close to 1, and a substantially smaller R$^2$ might indicate serious problems with the experiment in which the data were generated.  On the other hand, in typical application in biology, pyschology, marketing and other domains, the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large.  In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R$^2$ value well below 0.1 might be more realistic.\"  \n",
    "Trevor Hastie, Robert Tibshirani, et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My R$^2$ Is One!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example re-inforces the point above from Professors Hastie and Tibshirani to consider your application (or use case).  In processes that change slowly, the predictive power of a model (or representation) should be very good, reflected in the R$^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Diagnostics to Assess Model Quality\n",
    "\n",
    "Adjusted R$^2$: A metric that captures the penalty in the use of a large number of features with little explanatory power.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is evidence of omitted variable bias (sometimes called the \"collinearity problem\")\n",
    "* Failing to include land_size has biased up the measured effect of unit_size because the two features are positively correlated.\n",
    "* The two features, however, have independent effects on sales prices.\n",
    "* In this example, we can compare the R-squared measures across the two models because one is nested in the other.  \n",
    "* We see that the R-squared rises from 0.302 to 0.387, indicating a considerable improvement in the fit of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More features\n",
    "* Now let's examine what happens when we account for the age of the dwelling at the date of sale.\n",
    "* Thoughts about how age might affect sales prices?\n",
    "* Negative impact: old houses (of equal size) have lower sales prices.\n",
    "* Note, however, that there is little impact on the effects of the other features when we add age to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator Variables: Supervised to Semi-supervised\n",
    "\n",
    "* Indicator variables capture potential 'discontinuities'.\n",
    "* Todt Hill is an upscale area in Staten Island.\n",
    "* Suppose we want to capture this feature of the data, which we can do using the indicator \"todt\". \n",
    "* 3.574e+05 = 357,400 dollar premium in Todt Hill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Non-Linearities to the Linear Model\n",
    "\n",
    "* Suppose we want to consider the potential of non-linearities in $y=f(x)$.\n",
    "* We can use the linear model to do so, but must consider the calculus behind it.\n",
    "* One conjecture is that age has a diminishing effect on sales prices.\n",
    "* How to capture this conjecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Interpret?\n",
    "\n",
    "A model with $K$ features, $x_{ik}$ and label $y_i$: $y_i=\\sum_{k=1}^Kx_{ik}\\cdot\\beta_k+\\epsilon_i = x_i^\\prime \\beta + \\epsilon_i$\n",
    "\n",
    "$\\frac{\\partial{E(y_i})}{\\partial{x_{ik}}}=\\hat{\\beta_k}$\n",
    "\n",
    "$\\frac{\\partial{E(\\text{sales price}})}{\\partial{\\text{age}}}=\\hat{\\beta_\\text{age}} + 2\\cdot\\hat{\\beta_\\text{age2}}\\cdot{\\text{age}} = 997 - 2\\cdot17\\cdot\\text{age} = 997 - 34\\cdot\\text{age}$\n",
    "\n",
    "The average age of a house in our data is 36 years, so $997 - 34*36 = 997 - 1224 = -227$.  Therefore, when a house hits its average age, its value is declining every year.  \n",
    "\n",
    "Indeed, we can solve the simple algebra problem, $997 - 34*age = 0$, for age to obtain $\\text{age}\\approx29$.  For the first 29 years, house prices on average rise for each year, and then begin to decline for subsequent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine these results.  \n",
    "* Prices appear to be linear in unit size.\n",
    "* Prices appear to be quadratic in land_size: a positive but diminishing effect.  At what point does the positive effect disappear.\n",
    "* Prices appear to be quadatric in age: initially rising and then falling in age as houses reach the sample's average age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Elasticities\n",
    "* It may be of use to engineer your features so that results are unit free.\n",
    "* The interpretation is: for at 1% change in the feature, what is the % change in the label.\n",
    "* This can be achieved by tranforming the feature and labels using logarithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's examine these results.\n",
    "\n",
    "* A 10% change in unit size increases sales price by 3.4%.\n",
    "* A 10% change in land size increases sales price by 2.6%.\n",
    "* A 10% change in age decreases sales prices by 0.5%.\n",
    "* Prices in Todt Hill are about 44% higher holding all else constant.\n",
    "* Is this model directly comparable to the one above?\n",
    "* No because we non-linearly transformed both the label and the features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
